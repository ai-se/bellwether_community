{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import SMOTE\n",
    "import feature_selector\n",
    "import DE\n",
    "import CFS\n",
    "import birch\n",
    "import metrics.abcd\n",
    "\n",
    "import metrices\n",
    "import measures\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(labels = ['Host','Vcs','Project','File','PL','IssueTracking'],axis=1)\n",
    "    df = df.dropna()\n",
    "    df = df[['TLOC', 'TNF', 'TNC', 'TND', 'LOC', 'CL', 'NStmt', 'NFunc',\n",
    "       'RCC', 'MNL', 'avg_WMC', 'max_WMC', 'total_WMC', 'avg_DIT', 'max_DIT',\n",
    "       'total_DIT', 'avg_RFC', 'max_RFC', 'total_RFC', 'avg_NOC', 'max_NOC',\n",
    "       'total_NOC', 'avg_CBO', 'max_CBO', 'total_CBO', 'avg_DIT.1',\n",
    "       'max_DIT.1', 'total_DIT.1', 'avg_NIV', 'max_NIV', 'total_NIV',\n",
    "       'avg_NIM', 'max_NIM', 'total_NIM', 'avg_NOM', 'max_NOM', 'total_NOM',\n",
    "       'avg_NPBM', 'max_NPBM', 'total_NPBM', 'avg_NPM', 'max_NPM', 'total_NPM',\n",
    "       'avg_NPRM', 'max_NPRM', 'total_NPRM', 'avg_CC', 'max_CC', 'total_CC',\n",
    "       'avg_FANIN', 'max_FANIN', 'total_FANIN', 'avg_FANOUT', 'max_FANOUT',\n",
    "       'total_FANOUT', 'NRev', 'NFix', 'avg_AddedLOC', 'max_AddedLOC',\n",
    "       'total_AddedLOC', 'avg_DeletedLOC', 'max_DeletedLOC',\n",
    "       'total_DeletedLOC', 'avg_ModifiedLOC', 'max_ModifiedLOC',\n",
    "       'total_ModifiedLOC','Buggy']]\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    fs = feature_selector.featureSelector()\n",
    "    df,_feature_nums,features = fs.cfs_bfs(df)\n",
    "    return df,features\n",
    "\n",
    "def apply_cfs(df):\n",
    "    y = df.Buggy.values\n",
    "    X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "    X = X.values\n",
    "    selected_cols = CFS.cfs(X,y)\n",
    "    cols = df.columns[[selected_cols]].tolist()\n",
    "    cols.append('Buggy')\n",
    "    return df[cols],cols\n",
    "    \n",
    "def apply_smote(df):\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def load_data(path,target):\n",
    "    df = pd.read_csv(path)\n",
    "    if path == 'data/jm1.csv':\n",
    "        df = df[~df.uniq_Op.str.contains(\"\\?\")]\n",
    "    y = df[target]\n",
    "    X = df.drop(labels = target, axis = 1)\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    return X,y\n",
    "\n",
    "# Cluster Driver\n",
    "def cluster_driver(df,print_tree = True):\n",
    "    X = df.apply(pd.to_numeric)\n",
    "    cluster = birch.birch(branching_factor=20)\n",
    "    #X.set_index('Project Name',inplace=True)\n",
    "    cluster.fit(X)\n",
    "    cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "    #cluster_tree = cluster.model_adder(cluster_tree)\n",
    "    if print_tree:\n",
    "        cluster.show_clutser_tree()\n",
    "    return cluster,cluster_tree,max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted(cluster_data_loc,metrices_loc,fold,data_location,bell0_loc):\n",
    "    train_data = pd.read_pickle(cluster_data_loc + '/train_data.pkl')\n",
    "    cluster,cluster_tree,max_depth = cluster_driver(train_data)\n",
    "    t_df = pd.DataFrame()\n",
    "    for project in train_data.index.values.tolist():\n",
    "        _s_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + project\n",
    "        s_df = prepare_data(_s_path)\n",
    "        t_df = pd.concat([t_df,s_df])\n",
    "    t_df.reset_index(drop=True,inplace=True)\n",
    "    d = {'buggy': True, 'clean': False}\n",
    "    t_df['Buggy'] = t_df['Buggy'].map(d)\n",
    "    t_df, g_s_cols = apply_cfs(t_df)\n",
    "    t_df = apply_smote(t_df)\n",
    "    train_y = t_df.Buggy\n",
    "    train_X = t_df.drop(labels = ['Buggy'],axis = 1)\n",
    "    clf_global = LogisticRegression()\n",
    "    clf_global.fit(train_X,train_y)\n",
    "    test_data = pd.read_pickle(cluster_data_loc + '/test_data.pkl')\n",
    "    #print(test_data)\n",
    "    test_projects = test_data.index.values.tolist()\n",
    "    goals = ['recall','precision','pf','pci_20','ifa']\n",
    "    levels = [2,1,0]\n",
    "    for _ in range(1):\n",
    "        results = {}\n",
    "        bellwether_models = {}\n",
    "        bellwether0_models = {}\n",
    "        bellwether0_s_cols = {}\n",
    "        bellwether_s_cols = {}\n",
    "        self_model = {}\n",
    "        self_model_test = {} \n",
    "        for level in levels:\n",
    "            test_data = test_data\n",
    "            predicted_cluster = cluster.predict(test_data,level)\n",
    "            #print(level,predicted_cluster)\n",
    "            for i in range(len(predicted_cluster)):\n",
    "                try:\n",
    "                    F = {}\n",
    "                    _F = {}\n",
    "                    b_F = {}\n",
    "                    g_F = {}\n",
    "                    r_F = {}\n",
    "                    c_id = predicted_cluster[i]\n",
    "                    s_project_df = pd.read_csv(cluster_data_loc + '/bellwether_cdom_' + str(level) + '.csv')\n",
    "                    if level == 1:\n",
    "                        s_project_df.rename(columns = {'Unnamed: 0':'id'},inplace = True)\n",
    "                    if level == 0:\n",
    "                        s_project = s_project_df.bellwether.values[0]\n",
    "                    else:\n",
    "                        s_project = s_project_df[s_project_df['id'] == predicted_cluster[i]].bellwether.values[0]\n",
    "                    if s_project not in bellwether_models.keys():\n",
    "                        s_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + s_project\n",
    "                        df = prepare_data(s_path)\n",
    "                        df.reset_index(drop=True,inplace=True)\n",
    "                        d = {'buggy': True, 'clean': False}\n",
    "                        df['Buggy'] = df['Buggy'].map(d)\n",
    "                        df, s_cols = apply_cfs(df)\n",
    "                        bellwether_s_cols[s_project] = s_cols\n",
    "                        df = apply_smote(df)\n",
    "                        y = df.Buggy\n",
    "                        X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "                        clf_bellwether = LogisticRegression()\n",
    "                        clf_bellwether.fit(X,y)\n",
    "                        bellwether_models[s_project] = clf_bellwether\n",
    "                    else:\n",
    "                        clf_bellwether = bellwether_models[s_project]\n",
    "                        s_cols = bellwether_s_cols[s_project]\n",
    "                        \n",
    "                    b_s_project_df = pd.read_csv(bell0_loc + '/bellwether_cdom_0.csv')\n",
    "                    b_s_project = b_s_project_df.bellwether.values[0]\n",
    "                    if b_s_project not in bellwether0_models.keys():\n",
    "                        b_s_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + s_project\n",
    "                        b_df = prepare_data(b_s_path)\n",
    "                        b_df.reset_index(drop=True,inplace=True)\n",
    "                        d = {'buggy': True, 'clean': False}\n",
    "                        b_df['Buggy'] = b_df['Buggy'].map(d)\n",
    "                        b_df, b_s_cols = apply_cfs(b_df)\n",
    "                        bellwether0_s_cols[b_s_project] = b_s_cols\n",
    "                        b_df = apply_smote(b_df)\n",
    "                        b_y = b_df.Buggy\n",
    "                        b_X = b_df.drop(labels = ['Buggy'],axis = 1)\n",
    "                        b_clf_bellwether = LogisticRegression()\n",
    "                        b_clf_bellwether.fit(X,y)\n",
    "                        bellwether0_models[b_s_project] = b_clf_bellwether\n",
    "                    else:\n",
    "                        b_clf_bellwether = bellwether0_models[b_s_project]\n",
    "                        b_s_cols = bellwether0_s_cols[b_s_project]\n",
    "\n",
    "                    d_project = test_projects[i]\n",
    "                    kf = StratifiedKFold(n_splits = 10)\n",
    "                    d_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + d_project\n",
    "                    test_df = prepare_data(d_path)\n",
    "                    test_df.reset_index(drop=True,inplace=True)\n",
    "                    d = {'buggy': True, 'clean': False}\n",
    "                    test_df['Buggy'] = test_df['Buggy'].map(d)\n",
    "                    #test_df, x_s_cols = apply_cfs(test_df)\n",
    "                    test_y = test_df.Buggy\n",
    "                    test_X = test_df.drop(labels = ['Buggy'],axis = 1)\n",
    "                    for train_index, test_index in kf.split(test_X,test_y):\n",
    "                        X_train, X_test = test_X.iloc[train_index], test_X.iloc[test_index]\n",
    "                        y_train, y_test = test_y[train_index], test_y[test_index]\n",
    "                        x_test_df = pd.concat([X_train,y_train], axis = 1)\n",
    "                        x_test_df = apply_smote(x_test_df)\n",
    "                        y_train = x_test_df.Buggy\n",
    "                        X_train = x_test_df.drop(labels = ['Buggy'],axis = 1)\n",
    "                        clf_self = LogisticRegression()\n",
    "                        clf_self.fit(X_train,y_train)\n",
    "                        self_model[d_project] = clf_self\n",
    "                        self_model_test[d_project] = [X_test,y_test]\n",
    "\n",
    "                        _test_df = pd.concat(self_model_test[d_project], axis = 1)\n",
    "                        _df_test_loc = _test_df.LOC\n",
    "                        _test_df_1 = copy.deepcopy(_test_df[s_cols])\n",
    "                        _test_df_2 = copy.deepcopy(_test_df)\n",
    "                        _test_df_3 = copy.deepcopy(_test_df[b_s_cols])\n",
    "                        _test_df_4 = copy.deepcopy(_test_df[g_s_cols])\n",
    "                        _test_df_5 = copy.deepcopy(_test_df)\n",
    "\n",
    "                        y_test = _test_df_1.Buggy\n",
    "                        X_test = _test_df_1.drop(labels = ['Buggy'],axis = 1)\n",
    "                        predicted_bellwether = clf_bellwether.predict(X_test)\n",
    "                        abcd = metrices.measures(y_test,predicted_bellwether,_df_test_loc)\n",
    "                        if 'f1' not in F.keys():\n",
    "                            F['f1'] = []\n",
    "                            F['precision'] = []\n",
    "                            F['recall'] = []\n",
    "                            F['g-score'] = []\n",
    "                            F['d2h'] = []\n",
    "                            F['pci_20'] = []\n",
    "                            F['ifa'] = []\n",
    "                            F['pd'] = []\n",
    "                            F['pf'] = []\n",
    "                        F['f1'].append(abcd.calculate_f1_score())\n",
    "                        F['precision'].append(abcd.calculate_precision())\n",
    "                        F['recall'].append(abcd.calculate_recall())\n",
    "                        F['g-score'].append(abcd.get_g_score())\n",
    "                        F['d2h'].append(abcd.calculate_d2h())\n",
    "                        F['pci_20'].append(abcd.get_pci_20())\n",
    "                        try:\n",
    "                            F['ifa'].append(abcd.get_ifa_roc())\n",
    "                        except:\n",
    "                            F['ifa'].append(0)\n",
    "                        F['pd'].append(abcd.get_pd())\n",
    "                        F['pf'].append(abcd.get_pf())\n",
    "\n",
    "                        try:\n",
    "                            y_test = _test_df_2.Buggy\n",
    "                            X_test = _test_df_2.drop(labels = ['Buggy'],axis = 1)\n",
    "                            predicted_self = clf_self.predict(X_test) \n",
    "                            abcd = metrices.measures(y_test,predicted_self,_df_test_loc)\n",
    "                            if 'f1' not in _F.keys():\n",
    "                                _F['f1'] = []\n",
    "                                _F['precision'] = []\n",
    "                                _F['recall'] = []\n",
    "                                _F['g-score'] = []\n",
    "                                _F['d2h'] = []\n",
    "                                _F['pci_20'] = []\n",
    "                                _F['ifa'] = []\n",
    "                                _F['pd'] = []\n",
    "                                _F['pf'] = []\n",
    "                            _F['f1'].append(abcd.calculate_f1_score())\n",
    "                            _F['precision'].append(abcd.calculate_precision())\n",
    "                            _F['recall'].append(abcd.calculate_recall())\n",
    "                            _F['g-score'].append(abcd.get_g_score())\n",
    "                            _F['d2h'].append(abcd.calculate_d2h())\n",
    "                            _F['pci_20'].append(abcd.get_pci_20())\n",
    "                            try:\n",
    "                                _F['ifa'].append(abcd.get_ifa_roc())\n",
    "                            except:\n",
    "                                _F['ifa'].append(0)\n",
    "                            _F['pd'].append(abcd.get_pd())\n",
    "                            _F['pf'].append(abcd.get_pf())\n",
    "                        except:\n",
    "                            _F['f1'].append(0)\n",
    "                            _F['precision'].append(0)\n",
    "                            _F['recall'].append(0)\n",
    "                            _F['g-score'].append(0)\n",
    "                            _F['d2h'].append(0)\n",
    "                            _F['pci_20'].append(0)\n",
    "                            _F['ifa'].append(0)\n",
    "                            _F['pd'].append(0)\n",
    "                            _F['pf'].append(0)\n",
    "\n",
    "                        b_y_test = _test_df_3.Buggy\n",
    "                        b_X_test = _test_df_3.drop(labels = ['Buggy'],axis = 1)\n",
    "                        predicted_bell0 = b_clf_bellwether.predict(b_X_test) \n",
    "                        abcd = metrices.measures(b_y_test,predicted_bell0,_df_test_loc)\n",
    "                        if 'f1' not in b_F.keys():\n",
    "                            b_F['f1'] = []\n",
    "                            b_F['precision'] = []\n",
    "                            b_F['recall'] = []\n",
    "                            b_F['g-score'] = []\n",
    "                            b_F['d2h'] = []\n",
    "                            b_F['pci_20'] = []\n",
    "                            b_F['ifa'] = []\n",
    "                            b_F['pd'] = []\n",
    "                            b_F['pf'] = []\n",
    "                        b_F['f1'].append(abcd.calculate_f1_score())\n",
    "                        b_F['precision'].append(abcd.calculate_precision())\n",
    "                        b_F['recall'].append(abcd.calculate_recall())\n",
    "                        b_F['g-score'].append(abcd.get_g_score())\n",
    "                        b_F['d2h'].append(abcd.calculate_d2h())\n",
    "                        b_F['pci_20'].append(abcd.get_pci_20())\n",
    "                        try:\n",
    "                            b_F['ifa'].append(abcd.get_ifa_roc())\n",
    "                        except:\n",
    "                            b_F['ifa'].append(0)\n",
    "                        b_F['pd'].append(abcd.get_pd())\n",
    "                        b_F['pf'].append(abcd.get_pf())\n",
    "                        \n",
    "                        \n",
    "                        g_y_test = _test_df_4.Buggy\n",
    "                        g_X_test = _test_df_4.drop(labels = ['Buggy'],axis = 1)\n",
    "                        predicted_global = clf_global.predict(g_X_test) \n",
    "                        abcd = metrices.measures(g_y_test,predicted_global,_df_test_loc)\n",
    "                        if 'f1' not in g_F.keys():\n",
    "                            g_F['f1'] = []\n",
    "                            g_F['precision'] = []\n",
    "                            g_F['recall'] = []\n",
    "                            g_F['g-score'] = []\n",
    "                            g_F['d2h'] = []\n",
    "                            g_F['pci_20'] = []\n",
    "                            g_F['ifa'] = []\n",
    "                            g_F['pd'] = []\n",
    "                            g_F['pf'] = []\n",
    "                        g_F['f1'].append(abcd.calculate_f1_score())\n",
    "                        g_F['precision'].append(abcd.calculate_precision())\n",
    "                        g_F['recall'].append(abcd.calculate_recall())\n",
    "                        g_F['g-score'].append(abcd.get_g_score())\n",
    "                        g_F['d2h'].append(abcd.calculate_d2h())\n",
    "                        g_F['pci_20'].append(abcd.get_pci_20())\n",
    "                        try:\n",
    "                            g_F['ifa'].append(abcd.get_ifa_roc())\n",
    "                        except:\n",
    "                            g_F['ifa'].append(0)\n",
    "                        g_F['pd'].append(abcd.get_pd())\n",
    "                        g_F['pf'].append(abcd.get_pf())\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        r_y_test = _test_df_5.Buggy\n",
    "                        r_X_test = _test_df_5.drop(labels = ['Buggy'],axis = 1)\n",
    "                        _count_major = Counter(y_train)\n",
    "                        _count_major = _count_major.most_common(1)[0][0]\n",
    "                        predicted_random = [_count_major]*r_X_test.shape[0]\n",
    "                        abcd = metrices.measures(r_y_test,predicted_random,_df_test_loc)\n",
    "                        if 'f1' not in r_F.keys():\n",
    "                            r_F['f1'] = []\n",
    "                            r_F['precision'] = []\n",
    "                            r_F['recall'] = []\n",
    "                            r_F['g-score'] = []\n",
    "                            r_F['d2h'] = []\n",
    "                            r_F['pci_20'] = []\n",
    "                            r_F['ifa'] = []\n",
    "                            r_F['pd'] = []\n",
    "                            r_F['pf'] = []\n",
    "                        r_F['f1'].append(abcd.calculate_f1_score())\n",
    "                        r_F['precision'].append(abcd.calculate_precision())\n",
    "                        r_F['recall'].append(abcd.calculate_recall())\n",
    "                        r_F['g-score'].append(abcd.get_g_score())\n",
    "                        r_F['d2h'].append(abcd.calculate_d2h())\n",
    "                        r_F['pci_20'].append(abcd.get_pci_20())\n",
    "                        try:\n",
    "                            r_F['ifa'].append(abcd.get_ifa_roc())\n",
    "                        except:\n",
    "                            r_F['ifa'].append(0)\n",
    "                        r_F['pd'].append(abcd.get_pd())\n",
    "                        r_F['pf'].append(abcd.get_pf())\n",
    "\n",
    "                    for goal in goals:\n",
    "                        if goal == 'g':\n",
    "                            _goal = 'g-score'\n",
    "                        else:\n",
    "                            _goal = goal\n",
    "                        if _goal not in results.keys():\n",
    "                            results[_goal] = {}\n",
    "                        if d_project not in results[_goal].keys():\n",
    "                            results[_goal][d_project] = []\n",
    "                        results[_goal][d_project].append(np.median(F[_goal]))\n",
    "                        results[_goal][d_project].append(np.median(b_F[_goal]))\n",
    "                        results[_goal][d_project].append(np.median(_F[_goal]))\n",
    "                        results[_goal][d_project].append(np.median(g_F[_goal])) \n",
    "                        results[_goal][d_project].append(np.median(r_F[_goal])) \n",
    "                except Exception as e:\n",
    "                    print(d_project,e)\n",
    "                    continue\n",
    "        for key in results:\n",
    "            df = pd.DataFrame.from_dict(results[key],orient='index')\n",
    "            if not Path(data_location).is_dir():\n",
    "                os.makedirs(Path(data_location))\n",
    "            df.to_csv(data_location + '/bellwether_' + key + '.csv')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cluster_id=0] N_children: 9 N_samples: 628\n",
      "> [cluster_id=1] N_children: 7 N_samples: 74\n",
      "> > [cluster_id=2] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 13\n",
      "> [cluster_id=9] N_children: 2 N_samples: 5\n",
      "> > [cluster_id=10] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=11] N_children: 0 N_samples: 4\n",
      "> [cluster_id=12] N_children: 9 N_samples: 94\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=14] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=20] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 9\n",
      "> [cluster_id=22] N_children: 0 N_samples: 2\n",
      "> [cluster_id=23] N_children: 0 N_samples: 4\n",
      "> [cluster_id=24] N_children: 0 N_samples: 1\n",
      "> [cluster_id=25] N_children: 4 N_samples: 50\n",
      "> > [cluster_id=26] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=27] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=28] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 18\n",
      "> [cluster_id=30] N_children: 18 N_samples: 201\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=35] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 7\n",
      "> [cluster_id=49] N_children: 20 N_samples: 197\n",
      "> > [cluster_id=50] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=55] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=64] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=65] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=66] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=67] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=68] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=69] N_children: 0 N_samples: 9\n",
      "jchassis.csv index 1 is out of bounds for axis 0 with size 1\n",
      "jchassis.csv index 1 is out of bounds for axis 0 with size 1\n",
      "jchassis.csv index 1 is out of bounds for axis 0 with size 1\n",
      "[cluster_id=0] N_children: 9 N_samples: 628\n",
      "> [cluster_id=1] N_children: 8 N_samples: 79\n",
      "> > [cluster_id=2] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=9] N_children: 0 N_samples: 6\n",
      "> [cluster_id=10] N_children: 2 N_samples: 3\n",
      "> > [cluster_id=11] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 2\n",
      "> [cluster_id=13] N_children: 9 N_samples: 91\n",
      "> > [cluster_id=14] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=20] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 12\n",
      "> [cluster_id=23] N_children: 0 N_samples: 2\n",
      "> [cluster_id=24] N_children: 0 N_samples: 4\n",
      "> [cluster_id=25] N_children: 0 N_samples: 1\n",
      "> [cluster_id=26] N_children: 5 N_samples: 57\n",
      "> > [cluster_id=27] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=28] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 5\n",
      "> [cluster_id=32] N_children: 17 N_samples: 201\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=35] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=49] N_children: 0 N_samples: 18\n",
      "> [cluster_id=50] N_children: 19 N_samples: 190\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=55] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=64] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=65] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=66] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=67] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=68] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=69] N_children: 0 N_samples: 7\n",
      "duplicati.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: True\n",
      "jfreechart.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "duplicati.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: True\n",
      "jfreechart.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "duplicati.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: True\n",
      "jfreechart.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "[cluster_id=0] N_children: 7 N_samples: 627\n",
      "> [cluster_id=1] N_children: 0 N_samples: 1\n",
      "> [cluster_id=2] N_children: 0 N_samples: 1\n",
      "> [cluster_id=3] N_children: 5 N_samples: 55\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 13\n",
      "> [cluster_id=9] N_children: 10 N_samples: 127\n",
      "> > [cluster_id=10] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=11] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=14] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 15\n",
      "> [cluster_id=20] N_children: 16 N_samples: 183\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=23] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=24] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=25] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=26] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=27] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=28] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=35] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 14\n",
      "> [cluster_id=37] N_children: 0 N_samples: 2\n",
      "> [cluster_id=38] N_children: 20 N_samples: 258\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=49] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=50] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=55] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campsoft.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "gtad.csv index 1 is out of bounds for axis 0 with size 1\n",
      "google-collections.csv index 1 is out of bounds for axis 0 with size 1\n",
      "plog4u.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "wpdev.csv index 1 is out of bounds for axis 0 with size 1\n",
      "campsoft.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "gtad.csv index 1 is out of bounds for axis 0 with size 1\n",
      "google-collections.csv index 1 is out of bounds for axis 0 with size 1\n",
      "plog4u.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "wpdev.csv index 1 is out of bounds for axis 0 with size 1\n",
      "campsoft.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "gtad.csv index 1 is out of bounds for axis 0 with size 1\n",
      "google-collections.csv index 1 is out of bounds for axis 0 with size 1\n",
      "plog4u.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "wpdev.csv index 1 is out of bounds for axis 0 with size 1\n",
      "[cluster_id=0] N_children: 6 N_samples: 627\n",
      "> [cluster_id=1] N_children: 0 N_samples: 1\n",
      "> [cluster_id=2] N_children: 20 N_samples: 212\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=9] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=10] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=11] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=14] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=20] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 4\n",
      "> [cluster_id=23] N_children: 7 N_samples: 60\n",
      "> > [cluster_id=24] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=25] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=26] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=27] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=28] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 17\n",
      "> [cluster_id=31] N_children: 4 N_samples: 43\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=35] N_children: 0 N_samples: 11\n",
      "> [cluster_id=36] N_children: 12 N_samples: 162\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 5\n",
      "> [cluster_id=49] N_children: 14 N_samples: 149\n",
      "> > [cluster_id=50] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=55] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 15\n",
      "gotjava.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "gotjava.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "gotjava.csv This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,12):\n",
    "    fold = str(i)\n",
    "    data_location = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/src/data/1385/new_bellwether_pre_re_pf_v2/exp_cdom2/fold_' + fold\n",
    "    cluster_data_loc = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/src/data/1385/new_bellwether_pre_re_pf_v2/2/fold_' + fold\n",
    "    metrices_loc = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted'\n",
    "    bell0_loc = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/src/data/1385/new_bellwether_pre_re_pf_v2/0/fold_' + str(fold)\n",
    "    results = get_predicted(cluster_data_loc,metrices_loc,fold,data_location,bell0_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
