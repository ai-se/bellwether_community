{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import SMOTE\n",
    "import feature_selector\n",
    "import DE\n",
    "import CFS\n",
    "import birch\n",
    "import metrics.abcd\n",
    "\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from threading import Thread\n",
    "from multiprocessing import Queue\n",
    "\n",
    "import metrices\n",
    "import measures\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "    def run(self):\n",
    "        #print(type(self._target))\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source1 = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted'\n",
    "if platform.system() == 'Darwin' or platform.system() == 'Linux':\n",
    "    _dir = data_source1 + '/'\n",
    "else:\n",
    "    _dir = data_source1 + '\\\\'\n",
    "projects = [f for f in listdir(_dir) if isfile(join(_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(labels = ['Host','Vcs','Project','File','PL','IssueTracking'],axis=1)\n",
    "    df = df.dropna()\n",
    "    df = df[['TLOC', 'TNF', 'TNC', 'TND', 'LOC', 'CL', 'NStmt', 'NFunc',\n",
    "       'RCC', 'MNL', 'avg_WMC', 'max_WMC', 'total_WMC', 'avg_DIT', 'max_DIT',\n",
    "       'total_DIT', 'avg_RFC', 'max_RFC', 'total_RFC', 'avg_NOC', 'max_NOC',\n",
    "       'total_NOC', 'avg_CBO', 'max_CBO', 'total_CBO', 'avg_DIT.1',\n",
    "       'max_DIT.1', 'total_DIT.1', 'avg_NIV', 'max_NIV', 'total_NIV',\n",
    "       'avg_NIM', 'max_NIM', 'total_NIM', 'avg_NOM', 'max_NOM', 'total_NOM',\n",
    "       'avg_NPBM', 'max_NPBM', 'total_NPBM', 'avg_NPM', 'max_NPM', 'total_NPM',\n",
    "       'avg_NPRM', 'max_NPRM', 'total_NPRM', 'avg_CC', 'max_CC', 'total_CC',\n",
    "       'avg_FANIN', 'max_FANIN', 'total_FANIN', 'avg_FANOUT', 'max_FANOUT',\n",
    "       'total_FANOUT', 'NRev', 'NFix', 'avg_AddedLOC', 'max_AddedLOC',\n",
    "       'total_AddedLOC', 'avg_DeletedLOC', 'max_DeletedLOC',\n",
    "       'total_DeletedLOC', 'avg_ModifiedLOC', 'max_ModifiedLOC',\n",
    "       'total_ModifiedLOC','Buggy']]\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    fs = feature_selector.featureSelector()\n",
    "    df,_feature_nums,features = fs.cfs_bfs(df)\n",
    "    return df,features\n",
    "\n",
    "def apply_cfs(df):\n",
    "    y = df.Buggy.values\n",
    "    X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "    X = X.values\n",
    "    selected_cols = CFS.cfs(X,y)\n",
    "    cols = df.columns[[selected_cols]].tolist()\n",
    "    cols.append('Buggy')\n",
    "    return df[cols],cols\n",
    "    \n",
    "def apply_smote(df):\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def load_data(path,target):\n",
    "    df = pd.read_csv(path)\n",
    "    if path == 'data/jm1.csv':\n",
    "        df = df[~df.uniq_Op.str.contains(\"\\?\")]\n",
    "    y = df[target]\n",
    "    X = df.drop(labels = target, axis = 1)\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    return X,y\n",
    "\n",
    "# Cluster Driver\n",
    "def cluster_driver(df,print_tree = True):\n",
    "    X = df.apply(pd.to_numeric)\n",
    "    cluster = birch.birch(branching_factor=20)\n",
    "    #X.set_index('Project Name',inplace=True)\n",
    "    cluster.fit(X)\n",
    "    cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "    #cluster_tree = cluster.model_adder(cluster_tree)\n",
    "    if print_tree:\n",
    "        cluster.show_clutser_tree()\n",
    "    return cluster,cluster_tree,max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict = pd.read_pickle('data/1385/projects/selected_attr.pkl')\n",
    "attr_df = pd.DataFrame.from_dict(attr_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cluster_id=0] N_children: 9 N_samples: 697\n",
      "> [cluster_id=1] N_children: 9 N_samples: 87\n",
      "> > [cluster_id=2] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=9] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=10] N_children: 0 N_samples: 4\n",
      "> [cluster_id=11] N_children: 2 N_samples: 4\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 3\n",
      "> [cluster_id=14] N_children: 10 N_samples: 103\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=20] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=23] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=24] N_children: 0 N_samples: 18\n",
      "> [cluster_id=25] N_children: 0 N_samples: 2\n",
      "> [cluster_id=26] N_children: 0 N_samples: 4\n",
      "> [cluster_id=27] N_children: 0 N_samples: 1\n",
      "> [cluster_id=28] N_children: 6 N_samples: 64\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 7\n",
      "> [cluster_id=35] N_children: 19 N_samples: 222\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=49] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=50] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 7\n",
      "> [cluster_id=55] N_children: 20 N_samples: 210\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=64] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=65] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=66] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=67] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=68] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=69] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=70] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=71] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=72] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=73] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=74] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=75] N_children: 0 N_samples: 9\n"
     ]
    }
   ],
   "source": [
    "cluster,cluster_tree,max_depth = cluster_driver(attr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellwether(selected_projects,all_projects):\n",
    "    final_score = {}\n",
    "    count = 0\n",
    "    for s_project in selected_projects:\n",
    "        try:\n",
    "            s_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + s_project\n",
    "            print(s_project)\n",
    "            df = prepare_data(s_path)\n",
    "            if df.shape[0] < 50:\n",
    "                continue\n",
    "            else:\n",
    "                count+=1\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "            d = {'buggy': True, 'clean': False}\n",
    "            df['Buggy'] = df['Buggy'].map(d)\n",
    "            df, s_cols = apply_cfs(df)\n",
    "            df = apply_smote(df)\n",
    "            y = df.Buggy\n",
    "            X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "            kf = StratifiedKFold(n_splits = 5)\n",
    "            score = {}\n",
    "            F = {}\n",
    "            for i in range(5):\n",
    "                for train_index, tune_index in kf.split(X, y):\n",
    "                    X_train, X_tune = X.iloc[train_index], X.iloc[tune_index]\n",
    "                    y_train, y_tune = y[train_index], y[tune_index]\n",
    "                    clf = LogisticRegression()\n",
    "                    clf.fit(X_train,y_train)\n",
    "                    destination_projects = copy.deepcopy(all_projects)\n",
    "                    #destination_projects.remove(s_project)\n",
    "                    for d_project in destination_projects:\n",
    "                        try:\n",
    "                            d_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + d_project\n",
    "                            _test_df = prepare_data(d_path)\n",
    "                            _df_test_loc = _test_df.LOC\n",
    "                            test_df = _test_df[s_cols]\n",
    "                            if test_df.shape[0] < 50:\n",
    "                                continue\n",
    "                            test_df.reset_index(drop=True,inplace=True)\n",
    "                            d = {'buggy': True, 'clean': False}\n",
    "                            test_df['Buggy'] = test_df['Buggy'].map(d)\n",
    "                            test_y = test_df.Buggy\n",
    "                            test_X = test_df.drop(labels = ['Buggy'],axis = 1)\n",
    "                            predicted = clf.predict(test_X)\n",
    "                            abcd = metrices.measures(test_y,predicted,_df_test_loc)\n",
    "                            F['f1'] = [abcd.calculate_f1_score()]\n",
    "                            F['precision'] = [abcd.calculate_precision()]\n",
    "                            F['recall'] = [abcd.calculate_recall()]\n",
    "                            F['g-score'] = [abcd.get_g_score()]\n",
    "                            F['d2h'] = [abcd.calculate_d2h()]\n",
    "                            F['pci_20'] = [abcd.get_pci_20()]\n",
    "                            F['ifa'] = [abcd.get_ifa()]\n",
    "                            F['pd'] = [abcd.get_pd()]\n",
    "                            F['pf'] = [abcd.get_pf()]\n",
    "                            _F = copy.deepcopy(F)\n",
    "                            if 'f1' not in score.keys():\n",
    "                                score[d_project] = _F\n",
    "                            else:\n",
    "                                score[d_project]['f1'].append(F['f1'][0])\n",
    "                                score[d_project]['precision'].append(F['precision'][0])\n",
    "                                score[d_project]['recall'].append(F['recall'][0])\n",
    "                                score[d_project]['g-score'].append(F['g-score'][0])\n",
    "                                score[d_project]['d2h'].append(F['d2h'][0])\n",
    "                                score[d_project]['pci_20'].append(F['pci_20'][0])\n",
    "                                score[d_project]['ifa'].append(F['ifa'][0])\n",
    "                                score[d_project]['pd'].append(F['pd'][0])\n",
    "                                score[d_project]['pf'].append(F['pf'][0])\n",
    "                        except Exception as e:\n",
    "                            print(\"dest\",d_project,e)\n",
    "                            continue\n",
    "                final_score[s_project] = score \n",
    "        except Exception as e:\n",
    "            print(\"src\",s_project,e)\n",
    "            continue\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bellwether(projects):\n",
    "    threads = []\n",
    "    results = {}\n",
    "    #projects = projects[0:10]\n",
    "    split_projects = np.array_split(projects, cores)\n",
    "    for i in range(cores):\n",
    "        print(\"starting thread \",i)\n",
    "        t = ThreadWithReturnValue(target = bellwether, args = [split_projects[i],projects])\n",
    "        threads.append(t)\n",
    "    for th in threads:\n",
    "        th.start()\n",
    "    for th in threads:\n",
    "        response = th.join()\n",
    "        results.update(response)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(selected_projects,cluster_id):\n",
    "    print(cluster_id)\n",
    "    final_score = run_bellwether(selected_projects)\n",
    "    data_path = Path('data/1385/exp1/1/' + str(cluster_id))\n",
    "    if not data_path.is_dir():\n",
    "        os.makedirs(data_path)\n",
    "    with open('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_default_bellwether.pkl', 'wb') as handle:\n",
    "        pickle.dump(final_score, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    df = pd.read_pickle('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_default_bellwether.pkl')\n",
    "    results_f1 = {}\n",
    "    results_precision = {}\n",
    "    results_recall = {}\n",
    "    results_g = {}\n",
    "    results_d2h = {}\n",
    "    results_pci_20 = {}\n",
    "    results_ifa = {}\n",
    "    results_pd = {}\n",
    "    results_pf = {}\n",
    "    for s_project in df.keys():\n",
    "        if s_project not in results_f1.keys():\n",
    "            results_f1[s_project] = {}\n",
    "            results_precision[s_project] = {}\n",
    "            results_recall[s_project] = {}\n",
    "            results_g[s_project] = {}\n",
    "            results_d2h[s_project] = {}\n",
    "            results_pci_20[s_project] = {}\n",
    "            results_ifa[s_project] = {}\n",
    "            results_pd[s_project] = {}\n",
    "            results_pf[s_project] = {}\n",
    "        for d_projects in df[s_project].keys():\n",
    "            results_f1[s_project][d_projects] = np.median(df[s_project][d_projects]['f1'])\n",
    "            results_precision[s_project][d_projects] = np.median(df[s_project][d_projects]['precision'])\n",
    "            results_recall[s_project][d_projects] = np.median(df[s_project][d_projects]['recall'])\n",
    "            results_g[s_project][d_projects] = np.median(df[s_project][d_projects]['g-score'])\n",
    "            results_d2h[s_project][d_projects] = np.median(df[s_project][d_projects]['d2h'])\n",
    "            results_pci_20[s_project][d_projects] = np.median(df[s_project][d_projects]['pci_20'])\n",
    "            results_ifa[s_project][d_projects] = np.median(df[s_project][d_projects]['ifa'])\n",
    "            results_pd[s_project][d_projects] = np.median(df[s_project][d_projects]['pd'])\n",
    "            results_pf[s_project][d_projects] = np.median(df[s_project][d_projects]['pf'])\n",
    "    results_f1_df = pd.DataFrame.from_dict(results_f1, orient='index')\n",
    "    results_precision_df = pd.DataFrame.from_dict(results_precision, orient='index')\n",
    "    results_recall_df = pd.DataFrame.from_dict(results_recall, orient='index')\n",
    "    results_g_df = pd.DataFrame.from_dict(results_g, orient='index')\n",
    "    results_d2h_df = pd.DataFrame.from_dict(results_d2h, orient='index')\n",
    "    results_pci_20_df = pd.DataFrame.from_dict(results_pci_20, orient='index')\n",
    "    results_ifa_df = pd.DataFrame.from_dict(results_ifa, orient='index')\n",
    "    results_pd_df = pd.DataFrame.from_dict(results_pd, orient='index')\n",
    "    results_pf_df = pd.DataFrame.from_dict(results_pf, orient='index')\n",
    "    \n",
    "    results_f1_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_f1.csv')\n",
    "    results_precision_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_precision.csv')\n",
    "    results_recall_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_recall.csv')\n",
    "    results_g_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_g.csv')\n",
    "    results_d2h_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_d2h.csv')\n",
    "    results_pci_20_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_pci_20.csv')\n",
    "    results_ifa_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_ifa.csv')\n",
    "    results_pd_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_pd.csv')\n",
    "    results_pf_df.to_csv('data/1385/exp1/1/' + str(cluster_id)  + '/1385_LR_bellwether_pf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 3\n",
    "def run_on_cluster():\n",
    "    cluster_ids = [2,3,4,5,6,7,8,9,10,15,16,17,18,19,20,21,22,23,24,29,30,31,32,33,34,36,37,38,39,40,41,42,43,\n",
    "                   44,45,46,47,48,49,50,51,52,53,54,56,57,58,\n",
    "                   59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75]\n",
    "    for ids in cluster_ids:\n",
    "        selected_projects = list(attr_df.iloc[cluster_tree[ids].data_points].index)\n",
    "        run(selected_projects,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "logicmail.csv\n",
      "jsecurity.csv\n",
      "wsmt.csvforester-atv.csv\n",
      "geotools.csv\n",
      "pustefix.csv\n",
      "dxengine.csv\n",
      "\n",
      "openefm.csv\n",
      "jgnash.csv\n",
      "jsidplay2.csv\n",
      "jmonkeyengine.csv\n",
      "dsworkbench.csv\n",
      "opendvr.csv\n",
      "mclient-mume.csv\n",
      "sidekar.csv\n",
      "servicestack.csv\n",
      "haggle.csv\n",
      "yarp0.csv\n",
      "e-bio-flow.csv\n",
      "adonthell.csv\n",
      "icescrum.csv\n",
      "gephex.csv\n",
      "libopenmetaverse.csv\n",
      "jedit.csv\n",
      "jgossipforum.csv\n",
      "mp-rechnungs-und-kundenverwaltung.csv\n",
      "mediate.csv\n",
      "freelords.csv\n",
      "codesmith.csv\n",
      "coot.csv\n",
      "mobicents.csv\n",
      "jahshaka.csv\n",
      "ingex.csv\n",
      "tmva.csv\n",
      "joustsim.csv\n",
      "jpaul.csv\n",
      "mule.csv\n",
      "synecdoche.csv\n",
      "open-media-library.csv\n",
      "cuberok.csv\n",
      "ambulant.csv\n",
      "reaper3d.csv\n",
      "matrex.csv\n",
      "neodatis-odb.csv\n",
      "rcp-company-uibindings.csv\n",
      "springside.csv\n",
      "benojt.csv\n",
      "jbasic.csv\n",
      "autofac.csv\n",
      "ngl.csv\n",
      "freedom-erp.csv\n",
      "scenemonitor.csv\n",
      "array4j.csv\n",
      "swtfox.csv\n",
      "jatlas.csv\n",
      "apertium.csv\n",
      "mycila.csv\n",
      "madman.csv\n",
      "jas.csv\n",
      "gabel.csv\n",
      "sblim.csv\n",
      "chaperon.csv\n",
      "jam-daq.csv\n",
      "exist.csv\n",
      "azureus.csv\n",
      "jchassis.csv\n",
      "emono.csv\n",
      "pokenetonline.csv\n",
      "piccolo2d.csv\n",
      "calcubetimer.csv\n",
      "chrysalis.csv\n",
      "wheat.csv\n",
      "primefaces.csv\n",
      "octet.csv\n",
      "thedata.csv\n",
      "avisynth2.csv\n",
      "jbpm.csv\n",
      "nunit.csv\n",
      "beast-mcmc.csv\n",
      "power-architect.csv\n",
      "javaprofiler.csv\n",
      "jmoney.csv\n",
      "twostep.csv\n",
      "log4net.csv\n",
      "google-collections.csv\n",
      "bigdata.csv\n",
      "jrefactory.csv\n",
      "11\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "atunes.csv\n",
      "gpsmid.csvemftriple.csv\n",
      "\n",
      "columba.csv\n",
      "14\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "empyrean.csv\n",
      "freebios.csvenlightenment.csv\n",
      "\n",
      "aztec.csvjmule.csv\n",
      "\n",
      "multitude.csvultimairis.csv\n",
      "\n",
      "gogoego.csv\n",
      "xbg.csv\n",
      "qpe.csv\n",
      "amanda.csv\n",
      "openxava.csv\n",
      "ifmo-game-1.csv\n",
      "patterntesting.csv\n",
      "gebr.csv\n",
      "empserver.csv\n",
      "gdis.csv\n",
      "httpcontentparser.csv\n",
      "voms.csv\n",
      "mbse.csv\n",
      "lmms.csv\n",
      "nutz.csv\n",
      "bdbbasic.csv\n",
      "pcsx2.csv\n",
      "jaxlib.csv\n",
      "xqilla.csv\n",
      "fm-classic.csv\n",
      "vars-redux.csv\n",
      "yale.csv\n",
      "echongl.csv\n",
      "lufa-lib.csv\n",
      "pokeglobal.csv\n",
      "bibedt.csv\n",
      "wxstudio.csv\n",
      "foursquared.csv\n",
      "clamav.csv\n",
      "pgui.csv\n",
      "wxjs.csv\n",
      "jhotdraw.csv\n",
      "jlinalg.csv\n",
      "snakeyaml.csv\n",
      "simplewebservices.csv\n",
      "micomt.csv\n",
      "gnupdate.csv\n",
      "tradelink.csv\n",
      "cortex-vfx.csv\n",
      "unflobtactical.csv\n",
      "superwaba.csv\n",
      "impala.csv\n",
      "biopax.csv\n",
      "xpontus.csv\n",
      "src xpontus.csv index 1 is out of bounds for axis 0 with size 1\n",
      "rsyslog.csv\n",
      "alsa.csv\n",
      "google-gears.csv\n",
      "ffigo.csv\n",
      "cascading.csv\n",
      "fritzing.csv\n",
      "baadengine.csv\n",
      "dataobjectsdotnet.csv\n",
      "acegisecurity.csv\n",
      "yafdotnet.csv\n",
      "gstreamer.csv\n",
      "wikipediardware.csv\n",
      "playerstage.csv\n",
      "wxcrp.csv\n",
      "loki-lib.csv\n",
      "avifile.csv\n",
      "wow-qrsk.csv\n",
      "ns-3-dev-def-routing.csv\n",
      "crux-framework.csv\n",
      "arianne.csv\n",
      "knowledge.csv\n",
      "openmeetings.csv\n",
      "gnome-jabber.csv\n",
      "werx.csv\n",
      "ppcboot.csv\n",
      "xapian.csv\n",
      "metacosm.csv\n",
      "qgis.csv\n",
      "massiv.csv\n",
      "jnode.csv\n",
      "chaosrts.csv\n",
      "plog4u.csv\n",
      "src plog4u.csv index 1 is out of bounds for axis 0 with size 1\n",
      "jgameai.csv\n",
      "kml.csv\n",
      "les-indemodables.csv\n",
      "psycle.csv\n",
      "bl-toolkit.csv\n",
      "rtrt-on-gpu.csv\n",
      "gnukeda.csv\n",
      "exl.csv\n",
      "zealos.csv\n",
      "repositorium.csv\n",
      "mplayer-ce.csv\n",
      "ildjit.csv\n",
      "phonon-vlc-mplayer.csv\n",
      "amateur-scrolls.csv\n",
      "etherboot.csv\n",
      "ng4j.csv\n",
      "jfreereport.csv\n",
      "arcallians.csv\n",
      "quarkplusplus.csv\n",
      "aufs.csv\n",
      "25\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "javagroups.csvmagicwars.csv\n",
      "\n",
      "26\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "esmf.csvpentahoanalysistool.csv\n",
      "\n",
      "turbotrader-bos.csv\n",
      "personalaccess.csv\n",
      "27\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "xmemcached.csv\n",
      "35\n",
      "starting thread  0\n",
      "starting thread  1\n",
      "starting thread  2\n",
      "starting thread  3\n",
      "starting thread  4\n",
      "starting thread  5\n",
      "starting thread  6\n",
      "starting thread  7\n",
      "heekscad.csv\n",
      "kwave.csv\n",
      "unimrcp.csv\n",
      "checker-framework.csvquickfast.csv\n",
      "badtrinity-zero.csv\n",
      "qtractor.csv\n",
      "zscreen.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_on_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 2\n",
    "def run_on_cluster():\n",
    "    cluster_ids = [1,11,14,25,26,27,35,55] # need to include cluster 1\n",
    "    for ids in cluster_ids:\n",
    "        selected_projects = list(attr_df.iloc[cluster_tree[ids].data_points].index)\n",
    "        run(selected_projects,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_1 = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(attr_df.iloc[cluster_tree[0].data_points].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1385/exp1/1385_cluster_0.pkl', 'wb') as handle:\n",
    "        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1385/exp1/1385_cluster_0.pkl', 'rb') as handle:\n",
    "    y = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1385/exp1/1/55/1385_LR_default_bellwether.pkl', 'rb') as handle:\n",
    "    y = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
